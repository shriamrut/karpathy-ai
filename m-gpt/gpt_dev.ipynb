{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBME_vfqbPRJ",
        "outputId": "f2b103d1-fe4c-4270-9afa-dd9caef236c4"
      },
      "id": "qBME_vfqbPRJ",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-26 05:40:10--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-03-26 05:40:10 (19.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "676fbd24",
      "metadata": {
        "id": "676fbd24"
      },
      "outputs": [],
      "source": [
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "b41aedc2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b41aedc2",
        "outputId": "fefdd2e9-a80f-4ffb-d9e9-fc59961bc53e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of the dataset 1115394\n"
          ]
        }
      ],
      "source": [
        "print('length of the dataset %s'%len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7d499d5b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d499d5b",
        "outputId": "6fccff8f-ed51-4451-fb20-d0124c782185"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(text[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "bc0d1cef",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc0d1cef",
        "outputId": "a4d10d52-b5ca-424b-f24f-2e61d20d8d64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "dcbee025",
      "metadata": {
        "id": "dcbee025"
      },
      "outputs": [],
      "source": [
        "stoi = {c:i for i,c in enumerate(chars)}\n",
        "itos = {i:c for i,c in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda s: ''.join([itos[c] for c in s])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0bdbaadd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bdbaadd",
        "outputId": "ef7cae05-d22b-4c10-d43d-dd010e15868f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n"
          ]
        }
      ],
      "source": [
        "print(encode(\"hii there\"))\n",
        "print(decode(encode(\"hii there\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "12413756",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12413756",
        "outputId": "4dc09422-7771-458b-c9b1-5c1233fbf87a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
              "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
              "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
              "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
              "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
              "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
              "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
              "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
              "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
              "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
              "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
              "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
              "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
              "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
              "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
              "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
              "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
              "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
              "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
              "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
              "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
              "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
              "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
              "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
              "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
              "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
              "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
              "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
              "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
              "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
              "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
              "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
              "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
              "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
              "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
              "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
              "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
              "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
              "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
              "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
              "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
              "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
              "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
              "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
              "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
              "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
              "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
              "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
              "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
              "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
              "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
              "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
              "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
              "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
              "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
              "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import torch\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "data[:1000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "170b75c7",
      "metadata": {
        "id": "170b75c7"
      },
      "outputs": [],
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "test_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c0662edb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0662edb",
        "outputId": "b5d3ea5e-9b59-4e4a-c27c-79a72b1237ec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "98d063d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98d063d8",
        "outputId": "2d123cd0-4898-44f6-a0f8-7a8f84ec2a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target is 47\n",
            "when input is tensor([18, 47]) the target is 56\n",
            "when input is tensor([18, 47, 56]) the target is 57\n",
            "when input is tensor([18, 47, 56, 57]) the target is 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target is 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target is 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target is 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target is 58\n"
          ]
        }
      ],
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target is {target}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "888e142a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "888e142a",
        "outputId": "0ba05c58-8eef-462a-aaa3-658d7f07be6f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "xbatch shape:  torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "outputs:\n",
            "ybatch shape:  torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "-----\n",
            "when input [24],  target  43\n",
            "when input [24, 43],  target  58\n",
            "when input [24, 43, 58],  target  5\n",
            "when input [24, 43, 58, 5],  target  57\n",
            "when input [24, 43, 58, 5, 57],  target  1\n",
            "when input [24, 43, 58, 5, 57, 1],  target  46\n",
            "when input [24, 43, 58, 5, 57, 1, 46],  target  43\n",
            "when input [24, 43, 58, 5, 57, 1, 46, 43],  target  39\n",
            "when input [44],  target  53\n",
            "when input [44, 53],  target  56\n",
            "when input [44, 53, 56],  target  1\n",
            "when input [44, 53, 56, 1],  target  58\n",
            "when input [44, 53, 56, 1, 58],  target  46\n",
            "when input [44, 53, 56, 1, 58, 46],  target  39\n",
            "when input [44, 53, 56, 1, 58, 46, 39],  target  58\n",
            "when input [44, 53, 56, 1, 58, 46, 39, 58],  target  1\n",
            "when input [52],  target  58\n",
            "when input [52, 58],  target  1\n",
            "when input [52, 58, 1],  target  58\n",
            "when input [52, 58, 1, 58],  target  46\n",
            "when input [52, 58, 1, 58, 46],  target  39\n",
            "when input [52, 58, 1, 58, 46, 39],  target  58\n",
            "when input [52, 58, 1, 58, 46, 39, 58],  target  1\n",
            "when input [52, 58, 1, 58, 46, 39, 58, 1],  target  46\n",
            "when input [25],  target  17\n",
            "when input [25, 17],  target  27\n",
            "when input [25, 17, 27],  target  10\n",
            "when input [25, 17, 27, 10],  target  0\n",
            "when input [25, 17, 27, 10, 0],  target  21\n",
            "when input [25, 17, 27, 10, 0, 21],  target  1\n",
            "when input [25, 17, 27, 10, 0, 21, 1],  target  54\n",
            "when input [25, 17, 27, 10, 0, 21, 1, 54],  target  39\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "block_size = 8\n",
        "batch_size = 4\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else test_data\n",
        "    ix = torch.randint(len(data)-block_size, \n",
        "                       (batch_size,1))\n",
        "    xb = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    yb = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "    return xb, yb\n",
        "xb, yb = get_batch('train')\n",
        "print(\"inputs:\")\n",
        "print(\"xbatch shape: \",(xb.shape))\n",
        "print(xb)\n",
        "print(\"outputs:\")\n",
        "print(\"ybatch shape: \",(yb.shape))\n",
        "print(yb)\n",
        "\n",
        "print(\"-----\")\n",
        "\n",
        "for b in range(batch_size):\n",
        "    for t in range(block_size):\n",
        "        context = xb[b,:t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input {context.tolist()},  target  {target.tolist()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "e07cc9ff",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e07cc9ff",
        "outputId": "71e6d78a-41cf-45d8-92bf-3a0dc4f3dfd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGp\n",
            "wnYWmnxKWWev-tDqXErVKLgJ\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "        \n",
        "    def forward(self, idx, targets = None):\n",
        "        logits = self.token_embedding_table(idx) # (B,T,C)\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B,T,C  = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, 1) # B,C\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # B,1\n",
        "            idx = torch.cat([idx, idx_next], dim=1) # (B, T+1)\n",
        "        return idx\n",
        "            \n",
        "            \n",
        "    \n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "27808fb8",
      "metadata": {
        "id": "27808fb8"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(m.parameters(), lr = 1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "5264e1dd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5264e1dd",
        "outputId": "f1cdff78-460a-4dde-f298-303d8dd9b5b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.382369041442871\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(10000):\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = m.forward(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "896f5702",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "896f5702",
        "outputId": "8e5dbc2d-efca-48c0-b109-bcccdd269f84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "lso br. ave aviasurf my, yxMPZI ivee iuedrd whar ksth y h bora s be hese, woweee; the! KI 'de, ulseecherd d o blllando;LUCEO, oraingofof win!\n",
            "RIfans picspeserer hee tha,\n",
            "TOFonk? me ain ckntoty ded. bo'llll st ta d:\n",
            "ELIS me hurf lal y, ma dus pe athouo\n",
            "BEY:! Indy; by s afreanoo adicererupa anse tecorro llaus a!\n",
            "OLeneerithesinthengove fal amas trr\n",
            "TI ar I t, mes, n IUSt my w, fredeeyove\n",
            "THek' merer, dd\n",
            "We ntem lud engitheso; cer ize helorowaginte the?\n",
            "Thak orblyoruldvicee chot, p,\n",
            "Bealivolde Th li\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "551722ec",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "551722ec",
        "outputId": "7466d287-f841-4054-afdc-a55928065ebc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2\n",
        "x = torch.randn(B, T, C)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "cd5881c7",
      "metadata": {
        "id": "cd5881c7"
      },
      "outputs": [],
      "source": [
        "xbow = torch.zeros((B, T, C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b, :t+1]\n",
        "        xbow[b,t] = torch.mean(xprev, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57f9b483",
      "metadata": {
        "id": "57f9b483"
      },
      "source": [
        "### math trick behind self attention - v2!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d0f36db1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0f36db1",
        "outputId": "3afabc20-e5be-456e-b8ff-25c9ecd481fb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "wei = torch.tril(torch.ones(T,T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B,T,T) @ (B, T, C) = (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "960ff1cc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "960ff1cc",
        "outputId": "501a1c30-7c68-4219-c4fc-c1c34da3eafb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]]), tensor([[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]]))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "xbow[0], xbow2[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adc58722",
      "metadata": {
        "id": "adc58722"
      },
      "source": [
        "## math trick behind self attention - v3!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "b49e127d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b49e127d",
        "outputId": "a4bf8779-9343-43bd-fea9-64522be71a34"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim = 1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a99b633",
      "metadata": {
        "id": "3a99b633"
      },
      "source": [
        "### math trick behind self attention, simple example !!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "577b108b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "577b108b",
        "outputId": "b13cd24f-5929-41a8-fce9-c70222a3bcb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3,3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('b=')\n",
        "print(b)\n",
        "print('c=')\n",
        "print(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3caa806",
      "metadata": {
        "id": "e3caa806"
      },
      "source": [
        "### One Self Headed Attention v4 : self attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "1d8f4082",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d8f4082",
        "outputId": "49b2b3eb-1b1a-427e-9ece-fc8be063c75b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "B,T,C = 4, 8 ,32\n",
        "x = torch.randn(B, T, C)\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x) # (B, T, 16)\n",
        "q = query(x)# (B, T, 16)\n",
        "wei = q @ k.transpose(-2, -1) #* head_size**0.5 # (B,T,16) @ (B,16,T) = (B, T, T); B - T^2 matrices\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim = -1)\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "out.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "916d7a3e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "916d7a3e",
        "outputId": "89a914a8-4bae-4aab-fb5b-d1a83be5aa64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "wei[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "a7b7de35",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7b7de35",
        "outputId": "4da76e99-546a-47cb-c4f6-6925354541e0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "import torch\n",
        "class BatchNorm1d:\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        self.eps = eps\n",
        "        self.gamma = torch.ones(dim)\n",
        "        self.beta = torch.zeros(dim)\n",
        "        \n",
        "        # buffers (trained with momentum update)\n",
        "        self.running_mean = torch.zeros(dim)\n",
        "        self.running_var = torch.ones(dim)\n",
        "        \n",
        "    def __call__(self, x):\n",
        "        xmean = x.mean(1, keepdim = True)\n",
        "        xvar = x.var(1, keepdim = True)\n",
        "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "        self.out = self.gamma * xhat + self.beta\n",
        "        return self.out\n",
        "    def parameters(self):\n",
        "        return [self.gamma, self.beta]\n",
        "    \n",
        "torch.manual_seed(1337)\n",
        "module = BatchNorm1d(100)\n",
        "x = torch.randn(32, 100)\n",
        "x = module(x)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "1d0ee64e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d0ee64e",
        "outputId": "d0940242-d119-4ee0-bcc0-c7b4a52db28e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "x[:,0].mean(), x[:, 0].std()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "f4d8be76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4d8be76",
        "outputId": "cbd5f77c-d2c2-4a26-f3bb-42a32e5a2fbb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "x[0, :].mean(), x[0, :].std()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size  = 64 # How many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_layer = 6\n",
        "n_head = 6\n",
        "dropout = 0.2\n",
        "#---------------------------------------\n",
        "\n",
        "print(f\"Device {device}\")\n",
        "torch.manual_seed(13337)\n",
        "\n",
        "#Post downloading the dataset\n",
        "with open('input.txt', 'r') as f:\n",
        "    text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = {c:i for i,c in enumerate(chars)}\n",
        "itos = {i:c for i,c in enumerate(chars)}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda s: ''.join([itos[c] for c in s])\n",
        "\n",
        "# split into train and test\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "test_data = data[n:]\n",
        "\n",
        "#data loading\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else test_data\n",
        "    ix = torch.randint(len(data)-block_size, \n",
        "                       (batch_size,1))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+1+block_size] for i in ix])\n",
        "    x,y = x.to(device), y.to(device)\n",
        "    return x,y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X,Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\"Transformer block: communication followed by computation\"\"\"\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MuliHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "    \n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd), # projection layer\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "    \n",
        "class MuliHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, \n",
        "                              n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim = -1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "       \n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias = False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias = False)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "\n",
        "        # compute attention score\n",
        "        k = self.key(x)\n",
        "        q = self.query(x)\n",
        "        wei = q @ k.transpose(-2, -1) * C**-0.5 # (B,T,C) @ (B, C, T) = (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim = -1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform weighted aggregation of values\n",
        "        v = self.value(x) # (B, T, T)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, T) = (B, T, T)\n",
        "        return out\n",
        "    \n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head = n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "        \n",
        "    def forward(self, idx, targets = None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device = device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B, T, C)\n",
        "        x = self.blocks(x) # (B, T, C)\n",
        "        x = self.ln_f(x) # (B, T, C)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B,T,C  = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get thre prediction\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, 1) # B,C\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # B,1\n",
        "            idx = torch.cat([idx, idx_next], dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr = learning_rate)\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    #sample batch\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    #evaluate the loss\n",
        "    logits, loss = m.forward(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(idx = context, max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkcDmS4xbq-q",
        "outputId": "e62cbe53-872e-4705-b306-78f7982ff425"
      },
      "id": "YkcDmS4xbq-q",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device cuda\n",
            "10.788929 M parameters\n",
            "step 0: train loss 4.3342, val loss 4.3338\n",
            "step 500: train loss 2.0096, val loss 2.0891\n",
            "step 1000: train loss 1.6009, val loss 1.7781\n",
            "step 1500: train loss 1.4371, val loss 1.6376\n",
            "step 2000: train loss 1.3432, val loss 1.5738\n",
            "step 2500: train loss 1.2829, val loss 1.5279\n",
            "step 3000: train loss 1.2270, val loss 1.5049\n",
            "step 3500: train loss 1.1856, val loss 1.4941\n",
            "step 4000: train loss 1.1459, val loss 1.4874\n",
            "step 4500: train loss 1.1092, val loss 1.4802\n",
            "step 4999: train loss 1.0750, val loss 1.4883\n",
            "\n",
            "\n",
            "ROMEO:\n",
            "Yet, my daughter, he hath bid it cliet me from friend.\n",
            "Thou, man, now hath specially for Age:\n",
            "I am son each I so many my name.\n",
            "\n",
            "ROMEO:\n",
            "I have near eye so befallish men,\n",
            "Are not patience.\n",
            "\n",
            "First Senator:\n",
            "Sir, come hither-,\n",
            "What will you stay as France; for now, not for them?\n",
            "\n",
            "TYBRUT:\n",
            "Standy, but point it, at poor tops it his heavolaty,\n",
            "Whose states face fear, counsel in abses\n",
            "Flizering thee talk'd goose: come there to go eld conquer\n",
            "One that is hands of hastiling insaulted part blood\n",
            "Of c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxCXWHnF56W1",
        "outputId": "d0588705-4ca5-4bcc-e2e7-ea74890256a9"
      },
      "id": "mxCXWHnF56W1",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "not your beating. O sigh Claucence\n",
            "Thou in peace, by the The deep of life treasure\n",
            "Of he's victory. Spied! now living here!\n",
            "Why, I will be indeed; and hear my chough,\n",
            "But and that apince hear for scarrboy.\n",
            "How and you have moved wary a worship?\n",
            "And many asks his friends, Princely, I will partly\n",
            "From Lord Augarina will not be dayn, I'll hear Hercy;\n",
            "And command him healthful were friends\n",
            "Of such halty's anjusty threen here; and therefore\n",
            "I pay Saint George\n",
            "Did mock from Bosarn as the plants of late.\n",
            "\n",
            "EDWARD:\n",
            "Thou art a Mtostretchedy, good Plantagenet\n",
            "Shall doth unton him oaks him with their ings,\n",
            "But this gentlemanly king. Thou and not command the sires\n",
            "Upon his goodness depen time, out of that the\n",
            "emped, thought such grace that mocks, or ediff; to have\n",
            "Your meet doth not avoid yours, in your commoss\n",
            "Whom noble promost former, but false to us;\n",
            "And though is done from you that tell you,\n",
            "As I so, I am forg ound and sainty, but holy disarved\n",
            "With man, howls I fain my is London had a soly-brave,\n",
            "And to the keeping King Henry will,\n",
            "Hither's love. I crave yet me here it now,\n",
            "To tell your hateful bedness hatefard with command.\n",
            "Farewell: mean me, and herse, that I stumb'd my join\n",
            "Of back'd upon these impelliances and words,\n",
            "Our ducession wid between waters be strilling blown,\n",
            "And all toad choose of him some gave him beat.\n",
            "Thy treasons One! Why date good tongue thee,\n",
            "Arrchbborn with his parasion! we may love be:\n",
            "Come, with which was link'd dearth damned fled\n",
            "In: decline, for they may stay withhout Romeo, Enger-beyond;\n",
            "Pardon themselves gave at water hence to thee;\n",
            "And these to the repretations are o'clamack to Rams,\n",
            "Yea, on good courtesy, were not stronger,\n",
            "We better tumbling with love me soundly,\n",
            "Unless age. hast pass'd them lose and loses.\n",
            "\n",
            "GELO:\n",
            "Spit thou hasty dead those unkiluder woes;\n",
            "For all quite that Rome is thank it now:\n",
            "To what I may not heaven fear.\n",
            "\n",
            "CLIFFORD:\n",
            "Prithee now, what thou hast put on 't, I must not\n",
            "I bese remore a king. Play, I say your hope; but be Pompey,\n",
            "You that power-fellow the supper on.\n",
            "The bebella was of York, were hither,\n",
            "Forbid in the happy mother gates; but when his good\n",
            "Prayer's pardon friends, Norfolk, eye, mother be flattering\n",
            "Deliying caused ere his appear. Let me buss yourself.\n",
            "\n",
            "CORIFFORD:\n",
            "Fie, Heresay; himselves are here, as I fear.\n",
            "Is it fair Juliet, and living it is well.\n",
            "Never be you would occaps from his is abend.\n",
            "Your ere and heretility lives.\n",
            "\n",
            "MAMILLIUS:\n",
            "Faith, in time, bring your precious!\n",
            "Courage Murderer, very good: oniblus dreams,\n",
            "And cere-beard my honourable choicaps, fairly,\n",
            "I jest not coget with consument: to anothert\n",
            "Upeously and times, give ours patience, go with the mouth;\n",
            "Went weep; then in him, teir theres to lovely speak;\n",
            "And grusting is beholding still tuf,\n",
            "He is cameless, and so his good\n",
            "To by thus false.\n",
            "\n",
            "CLAUDIO:\n",
            "\n",
            "ISABELLA:\n",
            "I'll be honest.\n",
            "\n",
            "All:\n",
            "Swear me, sir, I\n",
            "Hence the matterch of the people: he wantons;\n",
            "Cries our hosts hearts of me, too soon;\n",
            "For 'tis the rage of deather\n",
            "Were he known like service: therefore,\n",
            "Which we have swored was to our by wooich,\n",
            "Being in Johe's coldly do we my brothers.\n",
            "\n",
            "CATESBUCKINGHAM:\n",
            "We have prevented with usneather flick,\n",
            "Henceful my late determine oath close\n",
            "Under the citteous chose, hearth serves;\n",
            "Still destinct his vow with Tybalt crown'd will\n",
            "Perceived by lord. God, sing him some nexture,\n",
            "Nor to pright him a king, feel him and dower!\n",
            "Sir he means--\n",
            "Fearing, as he is and here pleasing crys.\n",
            "\n",
            "First Watchman:\n",
            "Fell, let him find tell me sort stay:\n",
            "He hath laid entreaty has he won that I was.\n",
            "\n",
            "Second Messenger:\n",
            "'Faire hath 'tis a kind of the dew\n",
            "To thee thus cherised adhes, and which sweeted\n",
            "Regamed since itself:\n",
            "We are the field and the renupial in reple,\n",
            "To make the general life in the peoplets,\n",
            "Gettere of good drops.\n",
            "\n",
            "LADY CAPULET:\n",
            "Let snow, I am thy lord.\n",
            "\n",
            "LADY CAPULET:\n",
            "Welcome, sirs, sir.\n",
            "\n",
            "Nurse:\n",
            "Liege:\n",
            "I wish him bury here he meet.\n",
            "\n",
            "Favours:\n",
            "For his answers, and in hand most souches, and stove forget.\n",
            "\n",
            "First Citizen:\n",
            "His hearts, both haste as comforts be your part,\n",
            "And we have brought you so her faint that doth,\n",
            "Aged plays call this person. There Lukel of Lewis speech:\n",
            "So distings much dream times and kined modes,\n",
            "Destemples, those bravely blood of a fall wratch\n",
            "Spring so therefore, to offend Gives\n",
            "And times by for losses wash,\n",
            "And drawn an oviden pricks his limb wife,\n",
            "He will be a lerking clop in him:\n",
            "That's what 'pointed on my side where\n",
            "We that our trade; your meet news at homely\n",
            "To call to bear him with France, come one\n",
            "That leads to direct hide the earth\n",
            "And prize Romanspury him, Camillo's rocksn,\n",
            "My lighter with winth thee henly low'd with it dems,\n",
            "And so ill did not fear excuse.\n",
            "Dost thou with tread for peevil my heel.\n",
            "\n",
            "CAMILLO:\n",
            "Aed you seem forth\n",
            "To seek fold than what give you prisoner eld.\n",
            "Even to't. But they to Ireland,\n",
            "That did says in your hope breath note?\n",
            "Come, I know it bring your pace;\n",
            "But's suggers with your nature: the traitol\n",
            "Gentle to have I yet never an unspeasure here\n",
            "To renich the emposter-sweet togeth the fall.\n",
            "She's a crown but a grow; for stricting and stays:\n",
            "Though boys arrived mighty king limbs slave,\n",
            "Plain on core the docth are bethal.\n",
            "\n",
            "ISABELLA:\n",
            "Tasters, think you mwedged.\n",
            "\n",
            "CLAUDIO:\n",
            "Well, yet you require to the lie!\n",
            "As mistress: well, age obey's word, the flattered traitor:\n",
            "Chaple is more creatures on is forget.\n",
            "My leaponing father moons; but being made\n",
            "Still recover here joy Rutclisbes.\n",
            "\n",
            "WARWICK:\n",
            "Thus, how be confine?\n",
            "\n",
            "CLARD III:\n",
            "From help o' mose another\n",
            "False injuration to my father.\n",
            "\n",
            "MOPHERCY:\n",
            "Welcome, this work is all discovereign both,\n",
            "With catterflorshicardly sivers and fright:\n",
            "And thongs in all feds that importuness\n",
            "That have envy honour'd his mortal dish ran\n",
            "Endurative to dying the prove Richard's longer.\n",
            "Farewell, do this accuse, villain\n",
            "Adacious inquire or sovereign of thy decrets.\n",
            "\n",
            "MARCHISAMUS:\n",
            "Granden, noble Montague. What, Master\n",
            "Master commas-day? for had have you put their heartily.\n",
            "\n",
            "First Senatorshy, thither.\n",
            "\n",
            "CORIOLANUS:\n",
            "I'ld not be behold of him, and he kings\n",
            "What end to the Cupid command.\n",
            "My lady and stirs, none like a\n",
            "hunger-heUped-excemence imported into a drew play; or\n",
            "Morerlisk's pace, nor will admire by my rue.\n",
            "\n",
            "Third Murderer:\n",
            "I sway, 'tis a parg that traitor; that I\n",
            "give mine to Romeo quick-tongue.\n",
            "\n",
            "MENENIUS:\n",
            "You speak me bringef; but not 'tis not you, farewell. With one this is\n",
            "of your equitor; Pome.\n",
            "\n",
            "MENENIUS:\n",
            "Well you yet again.\n",
            "\n",
            "WARWICK:\n",
            "Then Lady, Is'tis against the grace?\n",
            "\n",
            "MARCIUS:\n",
            "Poor presson:\n",
            "The gods work I, indeed!\n",
            "Was your clack again, let ink king. Canst off\n",
            "The city?\n",
            "\n",
            "SICINIUS:\n",
            "You madam?\n",
            "\n",
            "MENENIUS:\n",
            "Why, what comes there?\n",
            "\n",
            "COMINIUS:\n",
            "That way provosted\n",
            "Him, say.\n",
            "\n",
            "CORIOLANUS:\n",
            "Herdity, you prepare know the return till.\n",
            "\n",
            "SICINIUS:\n",
            "Sistir, now is Colifford; the goes shall be found.\n",
            "\n",
            "BRUUTUS:\n",
            "Tiginst it with nothing coloves--\n",
            "That seeing ironsly, live these thee todards.\n",
            "\n",
            "\n",
            "SICINIUS:\n",
            "It was deceased, then and lips thee hangs,\n",
            "Stubb'd asides; and the norfelty as your thief,\n",
            "Some praise rock night sings, greative and him;\n",
            "And zorces with have got these soldiers:\n",
            "Myself in welcome, give these bridgings saws,\n",
            "My ll-bes giving best account stop.\n",
            "\n",
            "Second Murderer:\n",
            "O moster!\n",
            "\n",
            "Senger:\n",
            "Off age planet!\n",
            "\n",
            "Secondcure!\n",
            "\n",
            "First Murderer:\n",
            "Sir;\n",
            "\n",
            "Kneeleding Thusdate, when they are all they shall pardon.\n",
            "\n",
            "CATESBY:\n",
            "My lords:\n",
            "I pardon Well, dear you not, good lord; your gift earth,\n",
            "Nor I'll lady the deel terror,\n",
            "Sittings and the deep weep gapssy. The coral\n",
            "dead made me for I must get age with other.\n",
            "\n",
            "BART:\n",
            "Ay, because of life, sick you love soon,\n",
            "And wife yet to the gold both you must sweet call\n",
            "To-day that hath head for lives, and tradings slept along,\n",
            "Sits lover within. Perdon, Lucio, come!\n",
            "\n",
            "First Soverse:\n",
            "By heaven, I hope, we shall avenite\n",
            "To your hands not fear\n",
            "I she wear along, as you know's as even got\n",
            "you have. I'll had he any Master Froth. What news,\n",
            "How shows much is be in gentlemans? Let Ogerd.\n",
            "\n",
            "GLOUCESTER:\n",
            "Grantle and guards him will bust! a siath a king,\n",
            "As apperous bare our envy, old Marchia;\n",
            "Thereol-pincely put to flattering itsely.\n",
            "you man, but my tale, cousin bend\n",
            "To be termity of the which shadest damain,\n",
            "Look such himself. Are money lords are marry.\n",
            "And if they brought.\n",
            "A crue lives put homility to speak unto me;\n",
            "And kill ethem in the sight,\n",
            "Wanting makes to him scrudge haze with gone.\n",
            "But in a scarit\n",
            "Which hath childnelishings proud; and this lawful,\n",
            "Where I may for my kinsman present, in many comfort\n",
            "from my heir chaitiers up all discled and Christ:\n",
            "Steel it out my knees and loves: I'ld be bolend in ax\n",
            "Sail: theseize say. My delights nothings is fly: and he\n",
            "Will I cry ying out of leave age and companion.\n",
            "What strim I have give minded friends? For hath been?\n",
            "What of what though you disdant you, on my it?\n",
            "\n",
            "ISABELLA:\n",
            "Why says, I had rats's the bringer wards:\n",
            "And, God piest me grieve him,\n",
            "To lose live keeps well sackly more and stood aught!\n",
            "Alack I do save heaven I'll forget.\n",
            "Thy friends renown falt sons more kings\n",
            "Condish father easy's tent, and makes:\n",
            "Obey more, meaning, father than\n",
            "Parking age; this if to say not by my distrange\n",
            "Doing me to age in at this strue-staying custoop:\n",
            "Cannive be holging boasting with his proud fire?\n",
            "And in haste hath questity and behold:\n",
            "When how he, were it wealih you, we love a word?\n",
            "\n",
            "BUCKINGHAM:\n",
            "I am little prescry to you live his appetite,\n",
            "Richard, purged minis' alms compiracious him,\n",
            "And I am helded the falsehood spilling peint.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Do you tell me, his gain companion,\n",
            "To sleep in his enemies, for your anshy,\n",
            "Sweet estimations of that topsaddes.\n",
            "\n",
            "PARISINA:\n",
            "No, by all miss that\n",
            "\n",
            "ISABELLA:\n",
            "Grument is all.\n",
            "\n",
            "CLAUDIO:\n",
            "Proclain, it is true, in which go\n",
            "IShould law; and, but it as I should not promise\n",
            "Were I read at true, but sincely to push by\n",
            "This all; to the pride, but merribes with you: what, up,\n",
            "Where you know, my Montague is not\n",
            "To Lamberla's Warshiuma well-burge and soldiers.\n",
            "\n",
            "WARWICK:\n",
            "O undong what dost islength to the wantire,\n",
            "That Henry and revenge an his torments;\n",
            "And by Southam Clarence to defer Hection,\n",
            "The same sl\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}